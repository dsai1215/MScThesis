\chapter{Conclusions and Future work}\label{conc}
\label{sec:conclusions}
In this thesis, we presented an improved Hessian estimation scheme for the 2RDSA algorithm \cite{prashanth2015rdsa}. The proposed scheme was shown to be provably convergent to the true Hessian. We proposed generalisations of the RDSA algorithm proposed in \cite{prashanth2015rdsa}. As a result of the generalisation, we are able to use a much larger class of distributions  for the perturbation random variables. The proposed generalisation inherits all the improved Hessian estimation  schemes proposed earlier. We also presented an improved Hessian estimation scheme for the 2SPSA-3 algorithm \cite{bhatnagar2015simultaneous} and for the special case of a quadratic objective, it resulted in a  convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. The advantage with 2RDSA-IH, 2SPSA-3-IH is that these schemes it require only 75\% of the simulation cost per-iteration for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. Numerical experiments demonstrated that 2RDSA-IH, 2SPSA-3-IH outperform both 2SPSA-IH and 2SPSA without the improved Hessian estimation procesdure. They also indicate that schemes with improved Hessian estimation outperform the respective regular schemes without the improved Hessian estimation procedure. 2RDSA-IH with asymmetric Bernoulli distribution is seen to perform the best overall.

As future work, it would be interesting to look into the following directions:
\begin{enumerate}
\item Derive finite time bounds that show a lower Hessian estimation error for 2RDSA-IH when compared to 2RDSA and 2SPSA. 
\item Stochastic Newton methods converge faster and  are often more accurate than simple gradient search schemes. However they are computationally much costlier than first-order methods. Since first-order methods exhibit slower convergence rate and heavily depend on the step-size selection one has to resort to second-order methods for improved performance. As a future work, it would be interesting to look at  methods such as momentum descent, conjugate gradient as well as, Hessian free optimization approaches which result in faster convergence for the first-order methods and are known to be computationally cheaper than second-order methods in the deterministic optimization scenario. It would be interesting to extend those methods to the stochastic optimization setting and prove that they exhibit  faster convergence than first-order methods and result in lower computational cost. 
\item Newton scheme is not popular due to expensive matrix inversion even in deterministic scenario. Quasi-Newton schemes given by the Broyden family are used instead. As a future work, it would be interesting to develop stochastic approximation versions of the quasi-Newton schemes.  
\item In the methods described in this thesis the perturbations used are random variables. In \cite{bhatnagar2003two}, for the first-order SPSA method,  construction of two different deterministic  perturbation schemes was proposed and  under certain conditions on the perturbation vectors and noise sequences, the algorithm is shown to converge. The idea behind using deterministic perturbations is to reduce the bias in the gradient estimates. It would be interesting to extend this construction to the RDSA scheme as well. And moreover this kind of construction for second-order methods is not available for both SPSA and RDSA. One can use the above ideas and show significant improvements to the existing methods.
\item From an application point of view it would be interesting to apply these methods to some interesting real world applications. It would be interesting to explore applications of  these methods  to the design of reinforcement learning algorithms.
\end{enumerate}