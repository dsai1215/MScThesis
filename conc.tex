\chapter{Conclusions and Future work}\label{conc}
\label{sec:conclusions}
We presented an improved Hessian estimation scheme for the 2RDSA algorithm \cite{prashanth2015rdsa}. The proposed scheme was shown to be provably convergent to the true Hessian. We proposed generalisation of RDSA algorithm proposed in \cite{prashanth2015rdsa}. As a result of generalisation we are able to use much larger class of distributions  for perturbation random variables. The proposed generalisation inherits all the improved Hessian estimation  schemes proposed earlier. We also presented an improved Hessian estimation scheme for 2SPSA-3 algorithm \cite{bhatnagar2015simultaneous} and as a special case, for a quadratic objective resulted in a  convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. The advantage with 2RDSA-IH, 2SPSA-3-IH is that it requires only 75\% of the simulation cost per-iteration for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. Numerical experiments demonstrated that 2RDSA-IH, 2SPSA-3-IH outperforms both 2SPSA-IH and 2SPSA without the improved Hessian estimation scheme. They also indicate that schemes with improved Hessian estimation outperforms their respective regular schemes without the improved Hessian estimation scheme. 2RDSA-IH with asymmetric Bernoulli distribution performing the best overall.

As future work, it would be interesting to derive finite time bounds that show a lower Hessian estimation error for 2RDSA-IH when compared to 2RDSA and 2SPSA. Stochastic Newton methods  methods converge faster and  are often more accurate than simple gradient search schemes. However they are computationally much costlier than first-order methods. Since first-order methods exhibits slower convergence rate and heavily depended on the step-size selection one has to resort to second-order methods. As a future work, it would be interesting to look at the methods like momentum descent, conjugate gradient methods, Hessian free optimization schemes which result in faster convergence for the first-order methods in deterministic optimization case. It would be interesting to extend those methods to stochastic optimization case and prove that they exhibits  faster convergence than first-oder methods. From application point of view it would be interesting to apply these methods to some interesting real world applications.
