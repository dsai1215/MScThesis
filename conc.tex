\chapter{Conclusions and Future work}\label{conc}
\label{sec:conclusions}
We presented an improved Hessian estimation scheme for the 2RDSA algorithm \cite{prashanth2015rdsa}. The proposed scheme was shown to be provably convergent to the true Hessian. We proposed generalisation of RDSA algorithm proposed in \cite{prashanth2015rdsa}. As a result of generalisation we are able to use much larger class of distributions  for perturbation random variables. The proposed generalisation inherits all the improved Hessian estimation  schemes proposed earlier. We also presented an improved Hessian estimation scheme for 2SPSA-3 algorithm \cite{bhatnagar2015simultaneous} and as a special case, for a quadratic objective resulted in a  convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. The advantage with 2RDSA-IH, 2SPSA-3-IH is that it requires only 75\% of the simulation cost per-iteration for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. Numerical experiments demonstrated that 2RDSA-IH, 2SPSA-3-IH outperforms both 2SPSA-IH and 2SPSA without the improved Hessian estimation scheme. They also indicate that schemes with improved Hessian estimation outperforms their respective regular schemes without the improved Hessian estimation scheme. 2RDSA-IH with asymmetric Bernoulli distribution performing the best overall.

As future work, it would be interesting to look into the following directions
\begin{enumerate}
\item To derive finite time bounds that show a lower Hessian estimation error for 2RDSA-IH when compared to 2RDSA and 2SPSA. 
\item Stochastic Newton methods  methods converge faster and  are often more accurate than simple gradient search schemes. However they are computationally much costlier than first-order methods. Since first-order methods exhibits slower convergence rate and heavily depended on the step-size selection one has to resort to second-order methods. As a future work, it would be interesting to look at the methods like momentum descent, conjugate gradient methods, Hessian free optimization schemes which result in faster convergence for the first-order methods and computationally cheaper than second-order methods in deterministic optimization case. It would be interesting to extend those methods to stochastic optimization case and prove that they exhibits  faster convergence than first-oder methods and results in lower computational cost. 
\item In the methods described in this thesis the perturbations used are random variables. In \cite{bhatnagar2003two}, for first-order SPSA method,  construction of two different deterministic  perturbation schemes were proposed and  under certain conditions on the perturbation vectors and noise sequences algorithm is shown to converge. The idea behind using deterministic perturbations is to reduce the bias in the gradient estimates. It would be interesting to extend this construction to RDSA scheme as well. And moreover this kind of construction for second-order methods is not available for both SPSA and RDSA. One can use the above ideas and show significant improvements to the existing methods.
\item From application point of view it would be interesting to apply these methods to some interesting real world applications. Note that these methods can be easily extended to many of reinforcement learning algorithms.
\end{enumerate}