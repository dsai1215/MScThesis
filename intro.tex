\chapter{Introduction}
Optimization problems can be classified into several branches depending upon the nature of the problem. Consider the following optimization problem which is to find a $\theta^{*}$ that minimizes the objective function $f(\theta)$:
%\begin{equation} 
\begin{align}\label{eq:opt}
 \underset{\theta \in \Theta}{\min} ~f(\theta),
\end{align}
%\end{equation}
where $f \colon \mathbb{R}^N \to \mathbb{R}$ is called the objective function, $\theta$ is tunable N-dimensional parameter and $\Theta \subseteq \mathbb{R}^N$ is the constraint set in which $\theta$ takes values. 
\section{Classification of optimization problems based on objective function}\label{sc:objclass}
If we have complete information about $f$ and its derivatives etc., and about the set $\Theta$ then \eqref{eq:opt} would be a \emph{deterministic optimization} problem. In the real world unfortunately, many problems do not fall in this class. However, Optimization problems involving uncertainties are very common in a variety of engineering disciplines such as transportation systems, manufacturing, networks, healthcare and finance.

The setting in \emph{stochastic optimization}, however presumes that we have little knowledge on the structure of $f$ and moreover $f$ cannot be obtained directly, but rather is an expectation of another quantity $h(\theta,\xi)$, to which we have access, i.e.,
\begin{align}\label{eq:stopt}
 f(\theta) \equiv E_{\xi}[h(\theta,\xi)], 
\end{align}
 where $\xi$ comprises the randomness in the system and one is allowed to observe only these $h(\theta,\xi)$ samples, largely  because one dose not have access to distribution of noise $\xi$. This kind of optimization problems are more challenging because of the added complexity of not knowing $f$ and to find $\theta^{*}$ only on the basis of aforementioned noisy samples. The large number of input variables and the lack of precise system model may prohibit analytical solution approaches and a viable alternative is to employ a simulation-based optimization approach. As illustrated in Figure \ref{fig:so}, the idea here is to simulate a few times the stochastic system under consideration until a good enough solution is obtained. A natural solution approach is to devise an algorithm that incrementally updates the parameter, say $\theta_n$, in the descent direction using the gradient and/or Hessian of the objective $f$. However, in practice, one can only obtain estimates of the function $f$ through black-box simulation and the challenge is to estimate the gradient and/or Hessian of $f$ from function samples. \par

\tikzstyle{block} = [draw, fill=white, rectangle,
   minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

 \begin{figure}[t]
    \centering
\scalebox{0.8}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {$\boldsymbol{\theta_n}$};
\node [block, fill=blue!20,right=0.6cm of theta,align=center] (sample) {\makecell{\textbf{Simulator}}}; 
\node [right=0.6cm of sample] (end) {$\boldsymbol{\mathbf{f(\theta_n) + \xi_n}}$};
\node [ above right= 0.6cm of end] (bias) {\textbf{Zero mean}};
\draw [->] (theta) --  (sample);
\draw [->] (sample) -- (end);
\path [darkgreen,->] (bias) edge [bend left] (end);
\end{tikzpicture}}
\caption{Simulation optimization}
\label{fig:so}
\end{figure}

Suppose the objective function $f$ has the form $f(\theta) = \sum_{i=1}^{n} E[h_{i}(X_{i})]$ , where $n$ denotes the number of stages and $X_{i}$ is the state of the underlying process in stage $i$ and $h_{i}$ denotes a stage and state dependent cost function. Then this type of optimization problems are called \emph{multi-stage problems}. Let $\theta = (\theta_{1},\ldots,\theta_{n})^{T}$,  with each $\theta_{j}$ being scalar and Let $X_{i}$ depend on $\theta_{1},\ldots,\theta_{i}$. The idea is that optimization can be done one stage at a time over $n$ stages after observing the state $X_{i}$ in each stage $i$. The value $\theta_{i}$ in stage $i$ has bearing on the cost of all subsequent stages $i+1,...,n$. This is  a problem of dynamic optimization. Approaches such as Dynamic programming can be used to solve this optimization problem.\par
Now we consider sub-class of multi-stage problems having infinite number of stages and having \emph{long-run average cost function} as objective. Objective function here is of the form:
\begin{align}\label{eq:long}
 f(\theta) = \lim_{n \to \infty} \frac{1}{n} ~ E \left[ \sum_{i=1}^{n} h_{i}(X_{i})\right ] ,
\end{align}
where $X_{i}$ is defined same as before and we assume it depends on parameter $\theta$. Objective function \eqref{eq:long} in most cases is not known analytically. In such cases it makes difficult for the search procedure to update the search parameter with out estimating the cost over infinite long trajectory. This kind of objective functions are very common in reinforcement learning applications. Another subclassifications of optimization problems is based on feasible region $\Theta$, see \eqref{eq:opt}.

\section{Classification of optimization problems based on feasible region}
Consider the basic optimization problem in \eqref{eq:opt}. Optimization problems can be classified based on the structure of feasible region $\Theta$. Where set $\Theta$ can be of following types:

\begin{inparaenum}[\bfseries (i)]
 \item \emph{Discrete}
\item \emph{Continuous}
\item \emph{Hybrid} (continuous in some dimensions and discrete in others).
\end{inparaenum}
Hence corresponding optimization problems are called discrete, continuous, hybrid optimization problems. These subclassifications are common to both deterministic and stochastic optimization problems presented in section \ref{sc:objclass}. 

\section{Optimization methods via simulation }\label{sc:dis}
Optimization via simulation is fast developing area for both research and practice. As mentioned before, for stochastic optimization problems simulation is a viable approach compared to solving them analytically. Multiple simulations replications must be performed in order to get good estimate of $E_{\xi}[h(\theta_i,\xi)]$, which is function value at $\theta_i$. The standard approach to estimate is by the sample mean
\begin{align}\label{eq:sm}
\overline f(\theta_i) = \frac{1}{n} ~  \sum_{j=1}^{n} h(\theta,\xi_j),
\end{align}
where $n$ is the number of simulation replications and $\xi_j$ is the j-th sample of randomness. As $n \to \infty$, $\overline f(\theta_i) \to E_{\xi}[h(\theta_i,\xi)]$. Primary concerns in stochastic optimization via simulation are 
\begin{inparaenum}[\bfseries (i)]
	\item $n$ must be large enough to get good estimate of  $E_{\xi}[h(\theta_i,\xi)]$, which has ultimate impact on the final solution that the optimization procedure need to find; and
	\item The above procedure must be applied for many different parameters ,i.e., $\theta$'s in order to find best $\theta$.
\end{inparaenum}
Hence one of the key criteria for comparing different simulation-based optimization approaches is by total number of simulations performed by them to attain specific function value. Note that different approaches may take different simulations per single iteration. In this section we present methods only for Continuous and Discrete optimization via simulation problems which often occur in practise. However, hybrid optimization problems are not frequently encountered. From  now on, we consider the setting of optimization problems mentioned in this section as the stochastic optimization setting presented in section \ref{sc:objclass}, i.e., when objective function is an expected value over certain noisy cost measurements. 
\subsection{Discrete optimization via simulation}
Optimization problems for which $\theta \in \Theta$ is discrete and integer ordered and $\Theta$ is a subset of $N$-dimensional integers are known as Discrete optimization via simulation problems. In the discrete case $\theta$ can take only countable (finite or infinite) number of values. Discrete optimization problems are some times called as \emph{combinatorial optimization} and have applications in resource allocation, network routing, policy planning etc.
In this section we consider the case of $\Theta$ being finite i.e., $\Theta = \{\theta_{1},\theta_{2},\ldots,\theta_{k}\}$ where $k \in \mathbb{N}$. Discrete optimization problems are divided into two sub-classes when feasible region is  either small (often less than 100 ) or large. we present optimization methods in literature separately for each class.\\
%\begin{enumerate}
\subsubsection{$\Theta$ is small}
In this case we can simulate all possible solutions and select the best among them. However, unlike in deterministic optimization, simulating only once for each parameter is not enough since the objective function is noisy in stochastic optimization setting. Hence the main question is how to conduct multiple simulation effectively for each parameter in order to determine best $\theta$.  Two popular methods in literature for this case are Ranking and Selection (R\&S)\cite{bech} and Multiple comparison procedures (MCP) \cite{mcp}. These methods are well suited for simulation since under lying assumptions such as normality and independence of observations can be met by careful selection procedures.
Traditional methods are applicable for maximum size of 20 while some recent techniques allow more. 
\begin{enumerate}

\item \textbf {Ranking and selection }
R\&S procedures are statistical procedures specially developed to select the best parameter or subset that contains the best parameter. Majority of work in R\&S is classified into  two categories. The first one is called \emph{Indifferent-zone ranking}\cite{bech,izrs} which is aimed  to select the best and another one is \emph{Subset selection}\cite{ssrs} where the procedures are designed to find subset consisting of best. Again there are two approaches for selecting the best problem : \emph{the frequentist approach} and \emph{ Bayesian approach}. One of the popular streams of Bayesian approach procedures is  optimal computing budget allocation (OCBA) procedures \cite{ocbabook,chen2000} in which simulation budget is allocated in a manner that maximises the posterior probability of correct selection. This method is widely considered the best procedure for small scale discrete optimization.

\item \textbf {MCPs}
Like R\&S, MCPs attempt to identify the best parameter. But MCPs approach optimization problem as a statistical inference problem and do not guarantee a decision. Three main classes of MCPs that are used in practise are \emph{Multiple Comparison approach} (MCA) , \emph{Multiple Comparison with the Best} (MCB) and \emph{Multiple Comparisons with Control} (MCC). The most popular among these is MCB. In particular, MCB looks at $f(\theta_{j}) - opt_{i \neq j} f(\theta_{i})$ for $j = 1,2,\ldots,k$ to determine that $j_{*}$ with $f(\theta_{j_{*}}) - opt_{ i \neq j_{*}} f(\theta_{i}) > 0$. Simultaneous confidence intervals $f(\theta_{j}) - opt_{i \neq j} f(\theta_{i})$ for $j = 1,2,\ldots,k$ can be used to determine $j_{*}$ by looking  for the confidence interval with lower confidence limit that is zero.
\end{enumerate}
For a comprehensive treatment of these methods one can refer \cite{swish,swish2}.
\subsubsection{$\Theta$ is large}
When $\Theta$ is large enumeration becomes too expensive to conduct. Some sort of search techniques like the ones in deterministic optimization must be applied to avoid simulating for all the parameters while ensuring a high chance of finding the best or good parameters. Some approaches in this category include the following : 
\begin{enumerate}
\item \emph{Model-based approaches:} Iterative algorithms using statistical methods search the feasible region to improve upon the candidate parameter. One can  use \emph{gradient/Hessian information} with respect to parameter to help the search similar to deterministic case. But due to lack of availability of function analytical form and distribution of noise one needs to estimate the gradient/Hessian from samples of function $f$. However gradient/Hessian  estimation could be  quite noisy due to stochastic nature involved  in these problems. There are several approaches  which are used in continuous case can also be used in this setting to estimate the gradient/Hessian when simulation model is treated as black box. We will present detailed literature survey of these approaches in sections \ref{sec:gradest} and \ref{sec:hesest}. 
\item \emph{Metaheuristic:} These are gradient free approaches. They include approaches such as genetic algorithms, evolutionary algorithms, simulated annealing \cite{simann}, tabu search \cite{tabu} and cross entropy \cite{crossent}. Most of these iterative algorithms start with an initial population of parameters and in each iteration elite parameters are selected from previous population and generates  better parameter population as search progresses. 
\end{enumerate}


\subsection{Continuous optimization via simulation}
Optimization problems for which $\theta \in \Theta$ is a vector of continuous decision variables and $\Theta$ is a convex subset of $\mathcal{R}^N$ are called continuous optimization via simulation  problems. In these problems $\theta$ can take uncountable number of values. These kind of problems frequently occur in applications such as model fitting, adaptive control, neural network training, signal processing etc. A natural solution approach is to devise an algorithm that incrementally updates the parameter, say $\theta_n$, in the descent direction using the gradient and/or Hessian of the objective $f$. Stochastic approximation algorithms are best suited and most studied and used algorithms for solving continuous optimization problems via simulation.\par
The \textbf{stochastic approximation algorithm} (SA) takes the following iterative form:
\begin{align}\label{saalg}
\theta_{n+1} = \Gamma_{\Theta} \left [\theta_{n} - a_{n} \widehat{\nabla} f(\theta_{n}) \right],
\end{align}
where $\theta_{n}$ is the solution found at iteration $n$. $ \widehat{\nabla} f(\theta_{n})$ is an estimate of the gradient $ {\nabla} f(\theta_{n})$ and $\{a_{n}\}$ is a sequence of positive reals satisfy following properties: $\sum_{n=1}^{\infty} a_{n}  = \infty$,  $\sum_{n = 1}^{\infty} a_{n}^{2} < \infty$, and $\Gamma_{\Theta}$ denotes the projection of a point outside $\Theta$ onto $\Theta$. Under appropriate conditions, as number of iterations goes to infinity one can guarantee convergence to the local minimum with probability one. The SA algorithm can be considered as a stochastic analog of gradient decent method which seeks the next solution along negative gradient direction. The main difference between the SA algorithm and steepest descent algorithm is even though gradient estimate in former algorithm is a noisy estimate it requires only weak assumptions (like $E \left[\widehat{\nabla} f(\theta_{n})\right] - \nabla f(\theta_{n}) \to 0 $ at a certain rate) for converging to local minimum. In practice, the performance of the SA algorithm is quite sensitive to the sequence $\{a_{n}\}$ for example in the case $ a_n = a/n$, the convergence is highly dependent on the choice of $a$. However, in practice, one can only obtain estimates of the function $f$ through black-box simulation and the challenge is to estimate the gradient and/or Hessian of $f$ from function samples.  In the following sections we will present a brief survey of existing methods to estimate gradient and Hessian from function samples.
%There are several Gradient estimation methods present in the literature. Some of them are Finite difference approximation (FDA) and SPSA techniques, Perturbation analysis (PA) and Likelihood ratio / Score function (LF/SF) are popular among those.
%
%Formally, given only noise-corrupted measurements of an objective function $f$, we want to solve the following problem:
%\begin{align}
%\mbox{Find } x^* = \arg\min_{x \in \R^N} f(x). \label{eq:pb}
%\end{align}
\section{Gradient estimation}\label{sec:gradest}
\subsection{Finite-difference stochastic approximation (FDSA)}
When the simulation model is treated as a block box, the traditional means of forming the estimate of gradient is by using finite-difference stochastic approximation (FDSA) method. There are mainly two variations in the FDSA scheme. First one is  two-sided gradient approximation which involve function measurements $f(\theta_n + \delta_n e_i)$ and $f(\theta_n - \delta_n e_i)$, where $e_i$ denotes $i$th column of a identity matrix of size $N \times N$. Denote these respective values by $y_{ni}^+$ and $y_{ni}^-$, i.e., 
$$
y_{ni}^+ = f(\theta_n+\delta_n e_i) + \xi_{ni}^+,\quad y_{ni}^- = f(\theta_n-\delta_n e_i) + \xi_{ni}^-.
$$
In the above, we assume the noise vector $(\xi_{ni}^+ - \xi_{ni}^-, i=1,2,\ldots,N)\tr$ is a martingale difference sequence for every $n \ge 0$, the sequence of the perturbation constants $\{\delta_n, n\ge 0\}$ is a positive and asymptotically vanishing sequence. The gradient estimate of objective function using this scheme is given by
\begin{align}\label{eq:twofdsa}
 \widehat{\nabla} f(\theta_{n}) = \left(
\begin{array}{c}
\frac{y_{n1}^+ - y_{n1}^-}{2\delta_n}\\
\vdots\\
\frac{y_{nN}^+ - y_{nN}^-}{2\delta_n}\\
\end{array}
\right)
\end{align}
Second one is  one-sided gradient approximations which involve function measurements $f(\theta_n+\delta_n e_i)$ and $f(\theta_n)$. Let us denote these values by  $y_{ni}^+$, $y_{n}$  respectively, i.e., 
$y_{ni}^+ = f(\theta_n+\delta_n e_i) + \xi_{ni}^+$, $y_{n} = f(\theta_n) + \xi_n$,  where we assume the noise terms $(\xi_{ni}^+ - \xi_{n}, i=1,2,\ldots,N)\tr$ satisfy the martingale difference sequence property for $n \ge 0$. The gradient estimate of objective function using this scheme is given by
\begin{align}\label{eq:onefdsa}
 \widehat{\nabla} f(\theta_{n}) = \left(
\begin{array}{c}
\frac{y_{n1}^+ - y_{n}}{\delta_n}\\
\vdots\\
\frac{y_{nN}^+ - y_{n}}{\delta_n}\\
\end{array}
\right)
\end{align}
It is important to determine conditions under which $\theta_n$ converges to $\theta^*$, when one uses estimates shown in \eqref{eq:twofdsa} or \eqref{eq:onefdsa} used in \eqref{saalg}. The convergence theory for FDSA algorithm is similar to convergence theory of Robbins and Monro root finding SA algorithm \cite{rm}. However difficulties arise due to a bias in gradient approximation, i.e. $\E \left[\widehat{\nabla} f(\theta_{n})|\F_n \right] - \nabla f(\theta_{n}) $, where $\F_n = \sigma(x_m, m\le n)$ denotes the underlying sigma-field. The conditions required for showing convergence of this algorithm is as follows
\begin{enumerate}[label=(\textbf{A\arabic*})]
\item $f:\R^N\rightarrow \R$ is three-times continuously differentiable\footnote{Here $\nabla^3 f(\theta) = \dfrac{\partial^3 f (\theta)}{\partial \theta\tr \partial \theta\tr \partial \theta\tr}$ denotes the third derivate of $f$ at $\theta$ and $\nabla^3_{i_1 i_2 i_3} f(\theta)$ denotes the $(i_1 i_2 i_3)$th entry of $\nabla^3 f(\theta)$, for $i_1, i_2, i_3=1,\ldots, N$.}  with $\left|\nabla^3_{i_1 i_2 i_3} f(\theta) \right| < \alpha_0 < \infty$, for $i_1, i_2, i_3=1,\ldots, N$ and for all $\theta\in \R^N$. 
\item $\{\xi_n^+,\xi_n^-, n=1,2,\ldots\}$ satisfy $\E\left[\left.\xi_n^+ - \xi_n^- \right| \F_n\right] = 0$.
\item For some $\alpha_1, \alpha_2, \zeta >0$ and for all $n,i$, 
$\E \left|\xi_n^{\pm}\right|^{2+\zeta} \le \alpha_1$, $\E \left|f(\theta_n\pm \delta_n e_i)\right|^{2+\zeta} \le \alpha_2$. 
%\item $\{d_n^i, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d. and independent of $\F_n$.
\item The step-sizes $a_n$ and perturbation constants $\delta_n$ are positive, for all $n$ and satisfy
$$a_n, \delta_n \rightarrow 0\text{ as } n \rightarrow \infty, 
\sum_n a_n=\infty \text{ and } \sum_n \left(\frac{a_n}{\delta_n}\right)^2 <\infty.$$
\item $\sup_n \left\| \theta_n \right\| < \infty$ w.p. $1$.
\end{enumerate}
Note that two-sided gradient estimation scheme requires $2N$ simulations of objective function in order to obtain single gradient estimate, where as one-sided gradient estimation scheme requires only $N+1$ simulations, where $N$ is the dimension of the vector $\theta$. Dependence on $N$ for number of simulations to get one gradient estimate makes this procedure highly computationally expensive when $N$ is large. Hence one requires gradient estimate procedure that is independent of dimension of $\theta$.
\subsection{Simultaneous perturbation stochastic approximation (SPSA)}
Simultaneous perturbation (SP) methods are a popular and efficient approach for estimating gradient/Hessian from function samples, especially in high dimensional problems - see \cite{bhatnagar-book} for a comprehensive treatment of this subject matter. Simultaneous perturbation stochastic approximation (SPSA) is a popular SP method. The first-order SPSA algorithm, henceforth referred to as 1SPSA, was proposed in \cite{spall2005introduction}.  1SPSA scheme for approximating gradient requires only two simulations of function measurements irrespective of parameter dimension. The idea in this scheme is to perturb all components of parameter randomly for getting two function measurements. The function measurements required for this scheme corresponds to $\theta_n + \delta_n \Delta_n$ and $\theta_n - \delta_n \Delta_n$, respectively, where $\Delta_n = (\Delta_{n1},\ldots,\Delta_{nN})\tr$ is any vector consists of i.i.d, mean-zero, symmetric random variables whose inverse moments are bounded. The simplest and most commonly used perturbation distribution of random variables being used are symmetric Bernoulli distribution with $\Delta_{ni} = \pm 1 \,\,w.p.\,\,1/2$, $i=1,\ldots,N,\,\, n\ge 0$. The gradient estimate of objective function using this scheme is as follows
\begin{align}\label{eq:spsa}
 \widehat{\nabla} f(\theta_{n}) = \left(
\begin{array}{c}
\frac{f(\theta_n+\delta_n \Delta_n)-f(\theta_n-\delta_n \Delta_n)}{2\delta_n \Delta_{n1}} + \frac{\xi_{n}^+ - \xi_{n}^-}{2\delta_n \Delta_{n1}}\\
\vdots\\
\frac{f(\theta_n+\delta_n \Delta_n)-f(\theta_n-\delta_n \Delta_n)}{2\delta_n \Delta_{nN}} +  \frac{\xi_{n}^+ - \xi_{n}^-}{2\delta_n \Delta_{nN}}\\
\end{array}
\right)
\end{align}
The reason why 1SPSA algorithm is valid gradient estimation scheme can be seen easily from Taylor expansions of $f(\theta_n+\delta_n \Delta_n$ and $f(\theta_n-\delta_n \Delta_n)$.
\begin{align*}
&f(\theta_n + \delta_n \Delta_n) = f(\theta_n) + \delta_n \Delta_n\tr \nabla f(\theta_n) + \frac{\delta_n^2}{2} \Delta_n\tr \nabla^2 f(\theta_n) \Delta_n + o(\delta_n^2)\\
&f(\theta_n - \delta_n \Delta_n) = f(\theta_n) - \delta_n \Delta_n\tr \nabla f(\theta_n) + \frac{\delta_n^2}{2} \Delta_n\tr \nabla^2 f(\theta_n) \Delta_n + o(\delta_n^2)
\end{align*}
From above two equations, the $i$th component of gradient approximation \eqref{eq:spsa} is given by
\begin{align}\label{eq:spsaexp}
\frac{f(\theta_n + \delta_n \Delta_n) - \theta_n - \delta_n \Delta_n)}{2\delta_n \Delta_{ni}} = \nabla_i f(\theta_n) + \sum\limits_{j=1, j \neq i}^N \frac{\Delta_{nj} }{\Delta_{ni}}  \nabla^2_{j} f(\theta_n)  + o(\delta_n)
\end{align}
Now the conditional expectation of the 1SPSA gradient estimate is given by 
\begin{align}\label{eq:spsacondmean}
\E\left[\frac{f(\theta_n + \delta_n \Delta_n) - \theta_n - \delta_n \Delta_n)}{2\delta_n \Delta_{ni}} | \F_n \right] = \nabla_i f(\theta_n) + o(\delta_n)
\end{align}

The asymptotic convergence of 1SPSA algorithm requires one extra assumption other than the conditions specified in FDSA scheme above. The extra condition for convergence of $\theta_n$ to $\theta^*$ as $n \to \infty$ is as follows
\begin{enumerate}[label=(\textbf{A\arabic*}),resume]
\item $\{\Delta_{ni}, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d., independent of $\F_n$ and for some $\alpha_0 > 0$ and $\forall n$, $\E (\Delta_{ni})^{-2} \le \alpha_0$, for $i = 1,2,\ldots,N$.
\end{enumerate}
Since 1SPSA randomly perturbing the parameter vector $\theta$, number of function measurements required is two, irrespective of the dimension of $\theta$, see that the numerator of \eqref{eq:spsa} is same in all $N$ components of $\widehat{\nabla} f(\theta_{n})$. However, Inspite of the fact that the 1SPSA requires only two function measurements compared to  $2N$  measurements required by FDSA, the asymptotic convergence rate is the same as the FDSA scheme.
\subsection{Random directions stochastic approximation (RDSA)}
A closely related algorithm to SPSA is random directions stochastic approximation (RDSA) \cite[pp.~58-60]{kushcla}. The gradient estimate in RDSA differs from that in SPSA, both in the construction as well as in the choice of random perturbations.  In \cite{kushcla}, the random perturbations for 1RDSA were generated by picking samples uniformly on the surface of a sphere and the resulting 1RDSA scheme was found to be  inferior to 1SPSA from an asymptotic convergence rate viewpoint - see \cite{chin1997comparative}.  
The RDSA estimate of the gradient is given by
\begin{align}
\label{eq:grad-unif}
\widehat\nabla f(\theta_n) = d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right].
\end{align}
where $y_{n}^+ = f(\theta_n+\delta_n d_n) + \xi_{n}^+,\quad y_{n}^- = f(\theta_n-\delta_n d_n) + \xi_{n}^-$ ,$d_n = (d_n^i,\ldots,d_n^N)\tr$, and $d_n^i$, $ i=1,\ldots,N$ are i.i.d random perturbations distributed uniformly on $N$-dimensional sphere with radius 1. Like 1SPSA, 1RDSA also requires only two function measurements irrespective of dimension of parameter $\theta$. 
The asymptotic convergence of 1RDSA algorithm also requires one extra assumption other than the conditions specified in FDSA scheme. 
\begin{enumerate}[label=(\textbf{A\arabic*}),resume]
\item $\{ d_{n}^i, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d., independent of $\F_n$, and $\forall n$, $\E (d_{n} d_{n}\tr) = I_{N \times N}$, and for some $\alpha_0 > 0$, $\E {d_{n}^i}^2 f(\theta_n \pm \delta_n d_n) \le \alpha_0$ for $i=1,2,\ldots,N$.
\end{enumerate}
Recent work in \cite{prashanth2015rdsa} attempts to bridge the gap between 1RDSA and 1SPSA in terms of improving the performance by incorporating random perturbations based on an asymmetric Bernoulli
distribution as well as those with the  i.i.d uniform distribution. However,  1SPSA was found to be still marginally better than 1RDSA.  
% On the other hand, results in \cite{prashanth2015rdsa} show that a second order RDSA approach (referred to as 2RDSA hereafter) can considerably outperform the corresponding higher order SPSA algorithm \cite{spall_adaptive} (referred to as 2SPSA hereafter). 
\section{Hessian estimation}\label{sec:hesest}
Stochastic Newton methods can counter the ill-conditioning of the objective $f$ as they incorporate second-order information into the update iteration given by   
\begin{align}
\label{eq:newton}
\theta_{n+1} = \theta_n - a_n (\overline H_n)^{-1}\widehat\nabla f(\theta_n), 
\end{align}
where $a_n$ is the step-size that satisfies standard stochastic approximation conditions (see (C5) in Section \ref{sec:2rdsa-results}), $\widehat\nabla f(\theta_n)$ and $\overline H_n$ are estimates of the gradient and Hessian, respectively. Thus, \eqref{eq:newton} can be considered as  the stochastic version of the well-known Newton method for optimization. Stochastic Newton methods are often more accurate than simple gradient search schemes.
%which are sensitive to the choice of the constant $a_0$ in the canonical step-size, $a_n= a_0/n$. The optimal (asymptotic) convergence rate is obtained only if $a_0 > 1/3 \lambda_0$, where $\lambda_0$ is the minimum eigenvalue of the Hessian of the objective function (see \cite{fabian1967stochastic}). However, this dependency is problematic, as $\lambda_0$ is unknown in a \textit{simulation optimization} setting. Hessian-based methods get rid of this dependency, while attaining the optimal rate (one can set $a_0=1$). An alternative approach to achieve the same effect is to employ Polyak-Ruppert averaging, which uses larger step-sizes and averages the iterates. However, iterate averaging is optimal only in an asymptotic sense. Finite-sample analysis (see Theorem 2.4 in \cite{fathi2013transport}) shows that the initial error (that depends on the starting point $x_0$ of the algorithm) is not forgotten sub-exponentially fast, but at the rate $1/n$ where $n$ is the number of iterations. 
%Thus, 
%the effect of averaging kicks in only after enough iterations have passed and the bulk of the iterates are centered around the optimum. 

In \cite{fabian}, an estimation scheme for $\overline H_n$ that uses $O(N^2)$ function samples per-iteration of \eqref{eq:newton} was proposed.
%while in \cite{ruppert} the Hessian is estimated assuming knowledge of objective function gradients. 

% In \cite{bhat1}, three different  Hessian estimation schemes 
% that require three, two, and one simulation(s)  have been proposed
% in the context of long-run average cost objectives. 
%The resulting algorithms incorporate two-timescale stochastic approximation, see Chapter 6 of \cite{borkar}.
% Certain three-simulation balanced simultaneous perturbation Hessian estimates have been
% proposed in \cite{sbpla}. In addition, certain procedures for Hessian inversion  that require
% lower computational effort have also been proposed, see also \cite{bhatnagar-book}.
%In \cite{spall-jacobian}, certain enhancements to the four-simulation Hessian estimates of
%\cite{spall_adaptive} using some feedback and weighting mechanisms
%have been proposed. 
% In \cite{bhat2}, Newton-based smoothed functional algorithms based on Gaussian perturbations
% have been proposed.  An overview of random search approaches (both gradient and Newton-based) involving both theory and application of these techniques is available in \cite{bhatnagar-book}.


\subsection{Second-order SPSA (2SPSA)}
The number of samples per-iteration for estimating Hessian was brought down to four, irrespective of dimension $N$, by the second-order SPSA algorithm (henceforth referred to as 2SPSA). The Hessian estimator is projected to the space of positive definite and symmetric matrices at each iterate for the algorithm to progress along a descent direction. In this scheme two independent perturbation sequences $\Delta_n = (\Delta_{n1},\ldots,\Delta_{nN})\tr$, $\widehat \Delta_n = (\widehat \Delta_{n1},\ldots,\widehat \Delta_{nN})\tr$, each consists of random variables satisfying conditions specified in 1SPSA scheme are used.
The 2SPSA algorithm obtains four function samples $y_n^{++}$, $y_n^+$,  $y_n^{-+}$ and $y_n^{-}$ at $\theta_n + \delta_n \Delta_n + \widehat \delta_n \widehat \Delta_n $, $\theta_n +\delta_n \Delta_n$, $\theta_n - \delta_n \Delta_n + \widehat \delta_n \widehat \Delta_n$ and $\theta_n - \delta_n \Delta_n$, where  the sequences of the perturbation constants $\{\delta_n, n\ge 0\}, \{\widehat \delta_n, n\ge 0\}$ is a positive and asymptotically vanishing sequence and
the random perturbations $\Delta_n, \widehat \Delta_n$ are such that $\{\Delta_{ni},\widehat\Delta_{ni}, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d. and independent of the noise sequence. And $y_n^{++} = f(\theta_n + \delta_n \Delta_n + \widehat \delta_n \widehat \Delta_n ) + \xi_n^{++}$, $y_n^+ = f(\theta_n+\delta_n d_n) + \xi_n^+$, $y_n^{-+} = f(\theta_n - \delta_n \Delta_n + \widehat \delta_n \widehat \Delta_n) + \xi_n^{+-}$ and $y_n^- = f(\theta_n-\delta_n d_n) + \xi_n^-$, where the noise terms $\xi_n^{++}, \xi_n^+, \xi_n^{-+}, \xi_n^-$ satisfy $\E\left[\left.\xi_n^{++} - \xi_n^+ - \xi_n^{-+} +  \xi_n^-\right| \F_n\right] = 0$ with $\F_n = \sigma(\theta_m,m\le n)$ denoting the underlying sigma-field. The $i,j$th entry of the Hessian estimate $\widehat H_n$ in this case is given by
\begin{align}
\label{eq:2spsahhat}
&\left(\widehat H_n\right)_{ij} = \left[\frac{1}{\Delta_{ni}\widehat \Delta_{nj}} + \frac{1}{\Delta_{nj}\widehat \Delta_{ni}} \right] \left[\dfrac{y_n^{++} - y_n^+ - y_n^{-+} + y_n^-}{4 \delta_n \widehat \delta_n}\right], 
\end{align}
The second-order SPSA algorithm performs an update iteration as follows:
\begin{align}
\label{eq:2spsa}
\theta_{n+1} = \theta_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(\theta_n), \\
\overline H_n = \frac{n}{n+1} \overline H_{n-1} + \frac{1}{n+1} \widehat H_n.\label{eq:2spsa-H}
\end{align}
In the above, 
\begin{itemize}
 \item $\widehat\nabla f(\theta_n)$ is the estimate of $\nabla f(\theta_n)$ and this corresponds to \eqref{eq:spsa} 
  \item $\widehat H_n$ is an estimate of the true Hessian ${\nabla}^2 f(\cdot)$ at $\theta_n$. 
 \item $\overline H_n$ is a smoothed version of $\widehat H_n$, which is crucial to ensure convergence. 
 \item $\Upsilon$ is an operator that projects a matrix onto the set of positive definite matrices. Update \eqref{eq:2spsa-H} does not necessarily ensure that $\overline H_n$ is invertible and without $\Upsilon$, the parameter update \eqref{eq:2spsa} may not move along a descent direction - see conditions (C7)  in Section \ref{sec:2rdsa-results}  for the precise requirements on the matrix projection operator.
\end{itemize}
It is important to determine conditions under which $\theta_n$ converges to $\theta^*$ and $\overline H_n \to H(\theta^*)$. Convergence theory for second-order methods varies significantly to that of first-order methods. One can see \cite{spall_adaptive} for detailed convergence results.
\subsection{Second-order RDSA (2RDSA)}
The basic algorithm in \eqref{eq:2spsa}--\eqref{eq:2spsa-H} is similar to the adaptive scheme analyzed by \cite{prashanth2015rdsa}. However, they have used RDSA by incorporating random perturbations based on an asymmetric Bernoulli
distribution as well as those with the  i.i.d uniform distribution for the gradient and Hessian estimates (see \eqref{eq:grad-ber} and \eqref{eq:2rdsa-estimate-ber}, \eqref{eq:grad-unif} and \eqref{eq:2rdsa-estimate-unif} for their respective estimates), while \cite{spall_adaptive} employs SPSA. Though 1SPSA was found to be still marginally better than 1RDSA, results in \cite{prashanth2015rdsa} show that a second order RDSA approach (referred to as 2RDSA hereafter) can considerably outperform the corresponding second order SPSA algorithm \cite{spall_adaptive}, while requiring only three simulations per iteration of \eqref{eq:newton}.
\section{Contributions of this thesis}
The following are the brief description of the contributions of this thesis:
\begin{enumerate}
\item Our work in chapter \ref{sec:2rdsa-ih} is centred on improving the 2RDSA scheme of \cite{prashanth2015rdsa} by 
\begin{enumerate}[label={\bf\Roman*}]
\item reducing the error in the Hessian estimate through a feedback term; and
\item optimizing the step-sizes used in the Hessian estimation recursion, again with the objective of improving the quality of the Hessian estimate.
\end{enumerate}

Items (I) and (II) are inspired by the corresponding improvements to the Hessian estimation recursion in the enhanced 2SPSA from \cite{spall-jacobian}. We shall refer to the latter algorithm as 2SPSA-IH. While item (II) above is a relatively straightforward migration to the 2RDSA setting, item (I) is a non-trivial contribution, primarily because the Hessian estimate in 2RDSA is entirely different from that in 2SPSA and the feedback term that we incorporate in 2RDSA to improve the Hessian estimate neither correlates with that in 2SPSA nor follows from the analysis in \cite{spall-jacobian}. 
The advantage with 2RDSA scheme along with proposed improvement to Hessian estimation (henceforth referred to as 2RDSA-IH) is that it requires only 75\% of the simulation cost per-iteration for 2SPSA-IH.

We establish that the proposed improvements to Hessian estimation in 2RDSA are such  that the resulting 2RDSA-IH algorithm is provably convergent, in particular, the Hessian estimate $\overline H_n$ of 2RDSA-IH converges almost surely to the true Hessian. 
%Moreover, we show, for the special case of a quadratic objective, that	2RDSA-IH results in a convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. 
Further, we show empirically that 2RDSA-IH outperforms both 2SPSA-IH of \cite{spall-jacobian} and regular 2RDSA of \cite{prashanth2015rdsa}. Our contribution is important because 2RDSA-IH, like 2RDSA, has lower simulation cost per iteration than 2SPSA and unlike 2RDSA, has an improved Hessian estimation scheme.
\item Our work in chapter  \ref{sec:2rdsa-gen} is centred on generalising the 2RDSA scheme of \cite{prashanth2015rdsa}. In \cite{prashanth2015rdsa}, 2RDSA scheme involving only two perturbation distributions was proposed, those are uniform and asymmetric Bernoulli distributions. In chapter \ref{sec:2rdsa-gen} , we proposed gradient and Hessian estimation schemes which are independent of the perturbation distributions. However, perturbations have to satisfy the i.i.d, mean-zero assumptions. Apart from these common assumptions  one additional assumption, which is the difference between the second moment square  and fourth moment should be non zero for Hessian estimation is also required. This generalisation of distributions of random variables for perturbations is important because it enhances the scope of improving the performance of the 1RDSA and 2RDSA schemes by incorporating several other distributions. Some well known distribution that can used for these schemes as a result of generalisation are Normal (0,1) and Cauchy distributions.
\item Our work in \ref{sec:2spsa-3-ih} is centred on improving the second order SPSA scheme with three simulations ( referred to as 2SPSA-3 hereafter), which was proposed in  \cite{bhatnagar2015simultaneous}. This improvements are as a result of incorporating the zero mean feedback term in Hessian estimate and optimizing the step-sizes used in the Hessian estimate recursion. Though these ideas are same as the ideas presented in this section for 2RDSA scheme, the contribution is non-trivial because the derivation of feedback term and Hessian estimate is entirely different from 2SPSA, 2RDSA improvements. Moreover, we show, for the special case of a quadratic objective and when there is no noise, that 2SPSA-3 scheme along with proposed improvement to Hessian estimation (henceforth referred to as 2SPSA-3-IH) results in a convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. The advantage with 2SPSA-3-IH is that it requires only 75\% of the simulation cost per-iteration for 2SPSA-IH.
\end{enumerate}

The rest of the thesis is organised as follows: In Chapter~\ref{sec:2rdsa-ih}, we describe the improved Hessian estimation scheme, which is incorporated into the  2RDSA algorithm from \cite{prashanth2015rdsa}. We also presented the theoretical results for the 2RDSA algorithm with  improved Hessian estimation and the results from numerical experiments. In Chapter~\ref{sec:2rdsa-gen}, we describe the generalised RDSA scheme for both the gradient and Hessian estimates and also present the theoretical convergence results as well. The work related to improving the Hessian estimation scheme 2SPSA-3 was presented in Chapter~\ref{sec:2spsa-3-ih}. In Chapter~\ref{sec:expts}, we provide simulation experiments for all the methods proposed in the previous chapters and finally, in  Chapter~\ref{sec:conclusions}, the concluding remarks and future directions were provided.

